{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"KART Documentation","text":""},{"location":"#general-description","title":"General Description","text":"<p>The project consists of equipping a competition kart with the ability to operate autonomously, integrating perception, control and actuation systems.</p>"},{"location":"#objectives","title":"Objectives","text":"<p>Develop a platform to prototype autonomous systems that could later be installed on the single\u2011seater race car, and can be used by teachers, students, or researchers to evaluate their algorithms with an outdoor robot, which was not an option until recently. This accelerates innovation and reduces iteration costs.</p>"},{"location":"#regulatory-requirements-and-limitations","title":"Regulatory Requirements and Limitations","text":"<p>This prototype is not intended to compete, so no specific racing regulations apply. During development we try to meet any relevant standards and document deviations. Manual driving capability must be preserved for supervised learning. Wherever possible we favour standardized kart components and only resort to custom solutions when justified.</p>"},{"location":"contact/","title":"Contact","text":"<p>You can reach the \u00dc Motorsport Formula Student team via email at marketingumotorsport@gmail.com.</p> <p>Our workshop is located at \u00dc\u2011Motorsport \u2013 C. del Molino, S/N, 28943 Fuenlabrada, Madrid (40.280670, -3.822346).</p> <p>Our website: https://u-motorsport.urjc.es/</p> <p>Social media - Instagram - LinkedIn - TikTok - X - Facebook</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#emergency-braking-system","title":"Emergency Braking System","text":""},{"location":"faq/#why-are-you-using-a-pneumatic-actuator-for-the-braking-system","title":"Why are you using a pneumatic actuator for the braking system?","text":"<p>If we need a force of 100\u202fkgf at a speed of 0.5\u202fm/s the power requirement is roughly 500\u202fW. An electric actuator capable of that is bulky and hard to source. A cable and pulley system with a motor and pressure feedback was considered, but cables cannot bend sharply. Since we already have a pneumatic piston from our sponsor Festo, it is the simplest solution.</p>"},{"location":"faq/#battery","title":"Battery","text":""},{"location":"faq/#why-is-the-battery-placed-at-the-back","title":"Why is the battery placed at the back?","text":"<p>The rear of the kart offered the most practical space for such a large and heavy component. Because the seat, wheels and pedals remain in their original positions, placing the pack behind the driver was the best compromise. Handling is less of a concern since the kart will usually run autonomously without a driver's weight.</p>"},{"location":"faq/#motor-choice","title":"Motor choice","text":""},{"location":"faq/#why-not-use-a-thermal-engine","title":"Why not use a thermal engine?","text":"<p>A combustion engine would add unnecessary complexity such as fuel, exhaust and cooling. Electric motors require little maintenance and are simpler to control electronically. Since this prototype focuses on sensors, actuators and software, the type of engine does not matter as long as the vehicle moves when throttle is applied.</p>"},{"location":"electronics/wiring/","title":"Wiring","text":"<p>TODO</p>"},{"location":"hardware/battery/","title":"Battery","text":"<p>To charge the battery with a bench power supply, see this tutorial: C\u00f3mo Cargar una Bater\u00eda de Litio con una Fuente</p> <p>The main pack uses Molicel P42A cells in a 13S4P configuration, providing a nominal voltage of about 48\u202fV. A separate 12\u202fV car battery supplies the sensors to remain compatible with the Formula Student car.</p> <p>For battery placement rationale see the FAQ.</p> Parameter Value BMS Cutoff voltage 39.0 V (13 * 3.0V) Configuration 13S4P (13 cells in series, 4 in parallel) Nominal voltage 48\u202fV Maximum charging voltage 54.6 V (13 * 4.2V) Minimum voltage 41.6 V (13 * 3.2V) Power Capacity 808 Wh (3.7V * 4.2 Ah * 13 * 4) Charge capacity 16.8 Ah (4 * 4200 mAh) Maximum continuous discharge current 180 A (4 * 45 A. 9828W at 100% charge!!) Maximum continuous charge current 32 A (4 * 8 A. about 1.7kW) Cell type Molicel P42A Cell capacity 4200 mAh (4.2 Ah) Cell nominal voltage 3.7 V Cell maximum voltage 4.2 V Cell minimum voltage 3.2 V"},{"location":"hardware/battery/#bms","title":"BMS","text":"<ul> <li>Jiabaida BMS,\u00a0100A BT UART,\u00a0NMC 6S-21S</li> <li>https://www.notion.so/BMS-Bater-a-Kart-JBD-16078747314380e68688c3ab787fc1f7?pvs=21</li> <li>https://es.aliexpress.com/item/1005007223779359.html </li> <li></li> </ul>"},{"location":"hardware/camera/","title":"Index","text":""},{"location":"hardware/camera/#zed2-camera-integration-documentation","title":"ZED2 Camera Integration Documentation","text":""},{"location":"hardware/camera/#overview","title":"Overview","text":"<p>This document describes the integration of the ZED2 stereo camera into the kart, its usage through the ROS 2 wrapper, and the combination with YOLOv5 for cone detection. </p>"},{"location":"hardware/camera/#official-resources","title":"Official Resources","text":"<ul> <li>ZED2 Camera Overview: https://www.stereolabs.com/zed-2/</li> <li>ZED ROS 2 Wrapper Documentation: https://docs.stereolabs.com/ros2/</li> </ul>"},{"location":"hardware/camera/#hardware-zed2-camera","title":"Hardware: ZED2 Camera","text":"<p>The ZED2 camera by Stereolabs is a stereo vision camera capable of providing:</p> <ul> <li>High-definition left and right stereo images</li> <li>Depth sensing</li> <li>3D point clouds</li> <li>Positional tracking (6DoF)</li> <li>Integrated IMU sensors (accelerometer, gyroscope, magnetometer)</li> <li>Environmental sensors (barometer, temperature sensor)</li> </ul>"},{"location":"hardware/camera/#ros-2-integration","title":"ROS 2 Integration","text":"<p>The ZED2 camera is integrated into the project using the official Stereolabs ZED ROS 2 Wrapper:</p> <ul> <li>GitHub: https://github.com/stereolabs/zed-ros2-wrapper</li> </ul>"},{"location":"hardware/camera/#installation-requirements","title":"Installation Requirements","text":"<p>To properly install and run the ZED ROS 2 Wrapper with the ZED2 camera, you must ensure the following dependencies and system configuration are in place.</p> <ul> <li> <p>Operating System</p> <ul> <li>Ubuntu 24.04 LTS is the recommended version for this setup.</li> <li>Other Ubuntu versions may be used; however, note that dependencies such as CUDA, TensorRT, ROS2, and the ZED SDK may require different versions and additional compatibility testing.</li> </ul> </li> <li> <p>ROS 2 Jazzy</p> <ul> <li>Install ROS 2 Jazzy by following the official instructions here: https://docs.ros.org/en/jazzy/Installation/Ubuntu-Install-Debs.html</li> </ul> </li> <li> <p>CUDA Toolkit (12.0 to 12.9)</p> <ul> <li>Install CUDA 12.x (any version from 12.0 to 12.9 is compatible).</li> <li>Download from the official NVIDIA website: https://developer.nvidia.com/cuda-downloads</li> </ul> </li> <li> <p>ZED SDK (v5.0)</p> <ul> <li>Download and install ZED SDK v5.0 for Ubuntu 24.04 with CUDA 12 and TensorRT 10 from the official release page: https://www.stereolabs.com/en-es/developers/release/5.0#82af3640d775</li> </ul> </li> <li> <p>TensorRT 10</p> <ul> <li>Download the TensorRT 10 <code>.deb</code> package for Ubuntu 24.04 + CUDA 12.9 from the official NVIDIA repository: https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.10.0/local_repo/nv-tensorrt-local-repo-ubuntu2404-10.10.0-cuda-12.9_1.0-1_amd64.deb</li> </ul> <p>After downloading the <code>.deb</code> file, run the following commands to install it:</p> </li> </ul> <pre><code>sudo dpkg -i nv-tensorrt-local-repo-ubuntu2404-10.10.0-cuda-12.9_1.0-1_amd64.deb\nsudo apt update\n</code></pre> <p>If you encounter GPG key errors, follow these additional steps:</p> <pre><code>sudo cp /var/nv-tensorrt-local-repo-ubuntu2404-10.10.0-cuda-12.9/*.gpg /usr/share/keyrings/\n\nsudo nano /etc/apt/sources.list.d/nv-tensorrt-local-repo-ubuntu2404-10.10.0-cuda-12.9.list\n</code></pre> <p>Replace the content of the file with:</p> <pre><code>deb [signed-by=/usr/share/keyrings/nv-tensorrt-local-CD20EDBE-keyring.gpg] file:///var/nv-tensorrt-local-repo-ubuntu2404-10.10.0-cuda-12.9 /\n</code></pre> <p>Then update again:</p> <pre><code>sudo apt update\n</code></pre> <p>This should resolve the key issues.</p> <p>Finally, install the required TensorRT runtime libraries:</p> <pre><code>sudo apt-get install libnvinfer10 libnvinfer-dev libnvinfer-plugin-dev python3-libnvinfer\n</code></pre> <ul> <li> <p>ZED ROS 2 Wrapper</p> <ul> <li>Clone and build the zed-ros2-wrapper package in your existing ROS 2 workspace:</li> </ul> </li> </ul> <pre><code>cd ~/ros2_ws/src\ngit clone https://github.com/stereolabs/zed-ros2-wrapper.git\ncd ..\nrosdep install --from-paths src --ignore-src -r -y\ncolcon build --symlink-install\n</code></pre> <p>Official repository: https://github.com/stereolabs/zed-ros2-wrapper</p> <p>Once all the dependencies are installed and the wrapper is successfully built, you should be able to launch the ZED2 ROS 2 node without issues.</p>"},{"location":"hardware/camera/#launching-the-camera","title":"Launching the Camera","text":"<p>The camera is launched using a provided launch file, typically:</p> <pre><code>ros2 launch zed_wrapper zed_camera.launch.py camera_model:=zed2\n</code></pre>"},{"location":"hardware/camera/#cone-detection-with-yolov5","title":"Cone Detection with YOLOv5","text":"<p>In this project, YOLOv5 is used to perform real-time cone detection on images captured by the ZED2 camera. The ZED ROS 2 Wrapper supports custom object detection models through ONNX integration, allowing you to run your own trained detectors such as YOLOv5 directly on the GPU using TensorRT for real-time inference.</p>"},{"location":"hardware/camera/#exporting-and-using-a-custom-yolov5-model","title":"Exporting and Using a Custom YOLOv5 Model","text":"<p>If you have trained a YOLOv5 model (e.g., for cone detection), follow these steps to integrate it into the ZED wrapper:</p> <ol> <li>Export the model to ONNX format:    You can do this using PyTorch and the YOLOv5 export tools (e.g., <code>export.py</code> script from the YOLOv5 repository):</li> </ol> <p>This will generate a <code>.onnx</code> file.</p> <ol> <li>Enable object detection in the ZED wrapper by editing the configuration file:</li> </ol> <p>Open your <code>common_stereo.yaml</code> (located in your ROS 2 workspace, inside <code>zed-ros2-wrapper/zed_wrapper/config</code>), and modify or add the following lines:</p> <p><code>yaml    object_detection:         od_enabled: true          model: 'CUSTOM_YOLOLIKE_BOX_OBJECTS'         custom_onnx_file: '$path to model'</code></p>"},{"location":"hardware/camera/#first-time-optimization","title":"First-Time Optimization","text":"<p>The first time you launch the node with your custom ONNX model, TensorRT will optimize the model for inference, which may take additional time (several seconds to minutes depending on the system). Subsequent runs will be much faster, as the optimized engine will be cached and reused.</p> <p>Once all dependencies are correctly installed and the YOLOv5 model is configured, you should be able to run real-time object detection with the ZED2 camera using ROS 2.</p>"},{"location":"hardware/computer/","title":"Computer","text":"<p>Our computer is based on the NVidia Jetson Orin Development Kit.</p>"},{"location":"hardware/computer/#links","title":"Links","text":"<ul> <li>Main Nvidia Jetson Orin page: https://developer.nvidia.com/embedded/learn/jetson-agx-orin-devkit-user-guide/developer_kit_layout.html</li> <li>Datasheets for NVidia Jetson Orin: https://developer.nvidia.com/embedded/downloads TODO link here to datasheets in the repo itself</li> </ul>"},{"location":"hardware/computer/#data","title":"Data","text":""},{"location":"hardware/computer/#power","title":"Power","text":"<ul> <li>Power consumption: </li> <li>Voltage range:</li> <li>Power connectors: USB-C, barrel TODO something, 5.5mm maybe + add inner diameter</li> </ul>"},{"location":"hardware/ebs/","title":"Emergency Braking System","text":"<p>The kart uses a pneumatic piston to depress the brake pedal when commanded. The reasoning behind this choice is explained in the FAQ.</p>"},{"location":"hardware/motor/","title":"Powertrain","text":""},{"location":"hardware/motor/#motor","title":"Motor","text":"<p>We use a BLDC motor from Kunray along with its supplied controller. The motivation for selecting an electric motor over a combustion engine is discussed in the FAQ.</p>"},{"location":"hardware/steering_angle_sensor/","title":"Steering Angle Sensor","text":"<p>Sensor used is the cheap AS5600 </p> <p>Intended for use with a diametrically magnetized magnet, but works with a normal one turned 90 degrees too. It may be a good idea to find bigger neodymium diametric magnets.</p> <p>Repo with basic code to read the steering angle sensor (Arduino HAL with VSCode Platformio, no IDE): https://github.com/rubenayla/bluepill-angle-arduino.git</p> <p> </p>"},{"location":"software/","title":"Software","text":"<p>The software documentation will include its own repository, since it must be common and independent from Kart or Formula Student car.</p> <ul> <li>2024 version of the software, using Python:<ul> <li>https://github.com/UM-Driverless/driverless.git</li> <li>Tutorial</li> </ul> </li> <li>2025 version of the software, using ROS:<ul> <li>https://github.com/UM-Driverless/KART_SW.git</li> <li>git@github.com:UM-Driverless/KART_SW.git</li> </ul> </li> <li></li> </ul>"}]}